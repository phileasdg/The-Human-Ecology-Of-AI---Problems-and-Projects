{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPT3: Generating Text with a Language Model\n",
    "\n",
    "GPT3 is a generative language model (GPT) that is trained on a large corpus of text. It allows you to generate text from a given **context**, where the context is **a prompt** or seed **which takes the form of a sequence of words**.\n",
    "\n",
    "You can read up on how to use GPT3 here: [OpenAI documentation](https://beta.openai.com/docs/introduction/overview).\n",
    "\n",
    "Unfortunately, OpenAI's documentation does not really explain how the model works behind the scenes, so before we start playing around with it, let's fist explore some of the concepts behind it, and how it works."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How was GPT3 trained?\n",
    "\n",
    "GPT3 was trained on a large corpus of text composed of data from the following sources:\n",
    "\n",
    "| Dataset      | Quantity of Tokens | Weight in Training Mix |\n",
    "|--------------|--------------------|------------------------|\n",
    "| Common Crawl | 410 billion        |                    60% |\n",
    "| WebText2     | 19 billion         |                    22% |\n",
    "| Books1       | 12 billion         |                     8% |\n",
    "| Books2       | 55 billion         |                     8% |\n",
    "| Wikipedia    | 3 billion          |                     3% |\n",
    "\n",
    "(Source: [Language models are few-shot learners, OpenAI](https://arxiv.org/abs/2005.14165))\n",
    "\n",
    "Researchers at OpenAI used a program called a tokenizer to break the text into a series of tokens.\n",
    "\n",
    "**Tokens are common sequences of characters found in text.** You can try out OpenAI's Tokenizer [here](https://beta.openai.com/tokenizer) to get a better idea of what it does.\n",
    "\n",
    "Here is an example of generated using OpenAI's Tokenizer: A tokenisation of an excerpt from the first tablet of the Epic of Gilgamesh:\n",
    "\n",
    "<img src=\"Epic_of_Gilgamesh_excerpt_tablet_1_tokens.png\" width=\"573\"/>\n",
    "\n",
    "From the tokenized text, the model was trained using a type of deep-learning neural network architecture called a transformer. I would show you a diagram of how it works, but I think that's a bit too technical for this tutorial. If you must take a look, here's a [link to the diagram](https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png), and here's a [link to the original paper](https://arxiv.org/abs/1706.03762) which introduced the transformer architecture.\n",
    "\n",
    "You may have read that GPT3 has 175 billion machine learning parameters and - if like me when you first read this, you didn't really know that much about machine learning - been impressed without really knowing what that meant.\n",
    "\n",
    "**In machine learning, a neural network is a web of connections between variables called nodes** (sometimes also called the **perceptrons**, which I think is extremely cool). Each node is connected to one or more other nodes, and the strengths of the connections are called **weights**, or **parameters**.\n",
    "\n",
    "When you read that GPT3 has 175 billion parameters, it means that the full model is web that has 175 billion connections between nodes.\n",
    "\n",
    "Neural networks are often represented schematically, using diagrams like this one from [this page](https://www.ibm.com/cloud/learn/neural-networks) of IBM's website:\n",
    "\n",
    "<img src=\"IBM_neural_network_diagram.png\" width=\"592\"/>\n",
    "\n",
    "You can see in the diagram above that there are much fewer nodes than there are parameters, which is generally the case for a neural network.\n",
    "\n",
    "Before we start playing around with the model, I have one more graph to show you:\n",
    "\n",
    "<img src=\"Human_ability_to_detect_model_generated_news_articles.png\" width=\"640\"/>\n",
    "\n",
    "(Source: [Language models are few-shot learners, OpenAI](https://arxiv.org/abs/2005.14165))\n",
    "\n",
    "This last graph shows how the ability of people to detect news articles generated by GPT3 goes down as the number parameters used in the model increases, so much so that at 175 billion parameters, humans are only able to detect news articles generated by GPT3 about 50% of the time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup:\n",
    "\n",
    "Let's first set up some functions in code that will help us work with GPT3 without too much fuss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import openai\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "\"\"\"\n",
    "set up gpt-3 API connection\n",
    "\"\"\"\n",
    "\n",
    "API_key = open(\"api_key.txt\", \"r\").read()\n",
    "def setup(key=API_key):\n",
    "    openai.api_key = key\n",
    "\n",
    "setup()\n",
    "\n",
    "\"\"\"\n",
    "get gpt-3 to continue a prompt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_response(prompt, engine=\"davinci\", temp=0.7, max_tok=64, top_p=1, freq_pen=0, pres_pen=0,\n",
    "                 return_whole_obj=False, echo_prompt=True):\n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,\n",
    "        prompt=prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tok,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=freq_pen,\n",
    "        presence_penalty=pres_pen\n",
    "    )\n",
    "    if return_whole_obj is True:\n",
    "        return response\n",
    "    else:\n",
    "        if echo_prompt is True:\n",
    "            return str(prompt + response[\"choices\"][0][\"text\"])\n",
    "        else:\n",
    "            return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "get gpt-3 to follow an instruction from a prompt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def follow_instruction(prompt, engine=\"davinci-instruct-beta-v3\", temp=0.7, max_tok=64, top_p=1, freq_pen=0, pres_pen=0,\n",
    "                 return_whole_obj=False, echo_prompt=True):\n",
    "    response = openai.Completion.create(\n",
    "        engine=engine,\n",
    "        prompt=prompt,\n",
    "        temperature=temp,\n",
    "        max_tokens=max_tok,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=freq_pen,\n",
    "        presence_penalty=pres_pen\n",
    "    )\n",
    "    if return_whole_obj is True:\n",
    "        return response\n",
    "    else:\n",
    "        if echo_prompt is True:\n",
    "            return str(prompt + \"\\n\" + response[\"choices\"][0][\"text\"])\n",
    "        else:\n",
    "            return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "\"\"\"\n",
    "get gpt-3 davinci-codex to continue produce python code\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def codex_response(prompt, temp=0.7, max_tok=64, top_p=1, freq_pen=0, pres_pen=0,\n",
    "                   return_whole_obj=False, echo_prompt=True):\n",
    "    prompt = str(\"\\\"\\\"\\\"\\n\" + prompt + \"\\n\\\"\\\"\\\"\")\n",
    "    response = get_response(prompt, engine=\"davinci-codex\", temp=temp, max_tok=max_tok,\n",
    "                            top_p=top_p, freq_pen=freq_pen, pres_pen=pres_pen,\n",
    "                            return_whole_obj=return_whole_obj, echo_prompt=echo_prompt)\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Examples:\n",
    "\n",
    "### Getting GPT-3 to continue a prompt:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Well well well... What a nice surprise to find you here, darling.\" She gave her a wicked smile, her red lips glistening with gloss. \"I was just... looking for something to wear tonight,\" she lied. \"It's a big night for daddy, you know.\"\n\n\"I know,\" she said, trying not"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = get_response(\"Well well well... What\")\n",
    "md(format(completion))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting GPT-3 to follow an instruction from a prompt:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Tell me a joke about robots.\n\n\nWhy did the robot cross the road?\n\nTo get to the other side!"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion = follow_instruction(\"Tell me a joke about robots.\")\n",
    "md(format(completion))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting GPT-3 to generate python code:\n",
    "Let's ask GPT-3 to write some code for us:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Plot a histogram of a normal distribution\n",
      "\"\"\"\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set()\n",
      "sns.set_style(\"white\")\n",
      "sns.set_context(\"talk\")\n",
      "sns.set_palette('husl')\n",
      "\n",
      "x = np.random.normal(loc=0, scale=1, size=1000)\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111)\n",
      "\n",
      "ax.hist(x, bins=50, color=\"b\", normed=True)\n",
      "\n",
      "ax.set_xlabel(\"X\")\n",
      "ax.set_ylabel(\"P(X)\")\n",
      "\n",
      "plt.show()\n"
     ]
    }
   ],
   "source": [
    "completion_code = codex_response(\"Plot a histogram of a normal distribution\", max_tok=64*5)\n",
    "\n",
    "print(completion_code)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plot a histogram of a normal distribution\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample of 1000 normally distributed data points\n",
    "mu = 500\n",
    "sigma = 80\n",
    "sample = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure()\n",
    "plt.hist(sample, bins=100)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}